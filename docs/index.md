---
layout: default
title: DGX Spark Performance Benchmarking
---

# DGX Spark Performance Benchmarking
## Container vs Native GPU Performance Analysis

---

## ğŸ¯ Project Overview

This project provides **reproducible benchmarks** to measure GPU performance differences between **containerized** and **native** execution environments on NVIDIA DGX Spark systems.

### Why This Matters

NVIDIA DGX Spark systems running TensorRT-LLM in the development container (`spark-single-gpu-dev`) are experiencing **~50% performance degradation** from expected throughput. This benchmark suite helps:

âœ… Quantify container overhead
âœ… Isolate performance bottlenecks
âœ… Provide reproducible test cases
âœ… Generate shareable results for NVIDIA

---

## ğŸ“Š Quick Results

> Results will be published here after benchmarking

### Expected Performance Patterns

Based on [prior research](reference_paper.html), we expect:

| Environment | Execution Speed | GPU Utilization | Trade-off |
|-------------|----------------|-----------------|-----------|
| **Native** | âš¡ Faster | Lower (45-60%) | Raw performance |
| **Container** | ğŸŒ Slower | Higher (80-95%) | Consistency & portability |

**Key Question:** Is container overhead the cause of 50% performance gap?

---

## ğŸš€ Getting Started

### Prerequisites

- NVIDIA DGX Spark (or similar GPU system)
- Docker with NVIDIA Container Toolkit
- CUDA 12.x and compatible drivers
- Python 3.10+

### Quick Start (5 minutes)

```bash
# Clone repository
git clone https://github.com/YOUR_USERNAME/benchmark-spark.git
cd benchmark-spark

# Run container benchmark
./scripts/run_container_benchmark.sh all

# Results saved to results/container/
```

[View Full Setup Guide â†’](setup.html)

---

## ğŸ“– Documentation

### Core Documentation

- **[Quick Start Guide](../QUICKSTART.html)** - Get running in 5 minutes
- **[Setup Guide](setup.html)** - Detailed installation instructions
- **[Methodology](methodology.html)** - Benchmarking approach and rationale
- **[Reference Paper](reference_paper.html)** - Academic background

### Benchmarks

1. **Matrix Multiplication** (Sanity Check)
   - 10,000 x 10,000 matrix
   - Validates basic GPU operations
   - Comparable to prior research

2. **LLM Inference** (Production Workload)
   - TensorRT-LLM with various configurations
   - Batch sizes: 1, 4, 16, 32
   - Sequence lengths: 128, 512, 2048 tokens
   - Measures: throughput, latency, GPU utilization

### Analysis Tools

- **Automated comparison** - Statistical analysis with t-tests
- **Visualizations** - Box plots, line charts, overhead graphs
- **Markdown reports** - Shareable results

---

## ğŸ¯ Key Features

### âœ… Reproducible
- Automated scripts
- Fixed random seeds
- Version-controlled configurations

### âœ… Comprehensive
- Multiple workload types
- Statistical significance testing
- GPU utilization monitoring

### âœ… Practical
- Real LLM inference workloads
- Production-relevant configurations
- Easy to share results

### âœ… Open Source
- MIT Licensed
- Community contributions welcome
- Documented methodology

---

## ğŸ“ˆ Benchmark Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  1. Container Benchmark (5-10 min)                         â”‚
â”‚     ./scripts/run_container_benchmark.sh                    â”‚
â”‚     â”œâ”€ Matrix multiplication                                â”‚
â”‚     â””â”€ LLM inference tests                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. Native Setup (30-60 min, one-time)                     â”‚
â”‚     ./scripts/setup_native.sh                               â”‚
â”‚     â”œâ”€ Install TensorRT-LLM                                 â”‚
â”‚     â””â”€ Build from source                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. Native Benchmark (5-10 min)                            â”‚
â”‚     ./scripts/run_native_benchmark.sh                       â”‚
â”‚     â”œâ”€ Same workloads                                       â”‚
â”‚     â””â”€ Direct GPU access                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. Analysis & Comparison (2-5 min)                        â”‚
â”‚     python analysis/compare_results.py                      â”‚
â”‚     â”œâ”€ Statistical tests                                    â”‚
â”‚     â”œâ”€ Visualizations                                       â”‚
â”‚     â””â”€ Overhead calculation                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Understanding Results

### Overhead Interpretation

| Overhead | Interpretation | Action |
|----------|---------------|---------|
| **< 10%** | âœ… Minimal | Container not the issue; check elsewhere |
| **10-20%** | âš ï¸ Moderate | Expected range; acceptable for most use cases |
| **20-40%** | âš ï¸ Significant | Optimization opportunities exist |
| **> 40%** | ğŸ”´ Critical | Configuration issue or dev container broken |

### What to Check Next

1. **GPU Utilization** - Is GPU fully loaded?
2. **Thermal Throttling** - Check temperatures
3. **Power Limits** - Verify power draw
4. **Driver Versions** - Match container and host
5. **Container Image** - Try production vs dev

---

## ğŸ“¦ Repository Structure

```
benchmark-spark/
â”œâ”€â”€ README.md                      # Project overview
â”œâ”€â”€ QUICKSTART.md                  # 5-minute start guide
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ benchmarks/                    # Benchmark scripts
â”‚   â”œâ”€â”€ simple_matmul.py          # Matrix multiplication
â”‚   â”œâ”€â”€ llm_inference.py          # LLM benchmark
â”‚   â””â”€â”€ config.yaml               # Test configurations
â”œâ”€â”€ scripts/                       # Runner scripts
â”‚   â”œâ”€â”€ setup_native.sh           # Native installation
â”‚   â”œâ”€â”€ run_container_benchmark.sh
â”‚   â””â”€â”€ run_native_benchmark.sh
â”œâ”€â”€ analysis/                      # Analysis tools
â”‚   â””â”€â”€ compare_results.py        # Statistical comparison
â””â”€â”€ docs/                          # Documentation
    â”œâ”€â”€ setup.md
    â”œâ”€â”€ methodology.md
    â””â”€â”€ reference_paper.md
```

---

## ğŸ¤ Contributing

Contributions welcome! Areas of interest:

- Additional benchmark workloads
- Optimization suggestions
- Bug fixes
- Documentation improvements
- Results from different hardware

[Open an Issue](https://github.com/YOUR_USERNAME/benchmark-spark/issues) | [Submit PR](https://github.com/YOUR_USERNAME/benchmark-spark/pulls)

---

## ğŸ“š Background Research

This project builds on academic research:

> **"Benchmarking GPU Passthrough Performance on Docker for AI Cloud System"**
> Sani et al., 2025
> Found 67% overhead on consumer GPU (RTX 3060)

[Read Full Paper Summary â†’](reference_paper.html)

**Key Differences:**
- Enterprise GPU (DGX) vs Consumer (RTX 3060)
- Production workload (LLM) vs Synthetic (matmul)
- Hardware-optimized container vs Generic

---

## ğŸ“„ License

MIT License - See [LICENSE](../LICENSE) for details

---

## ğŸ“ Contact & Support

- **Issues:** [GitHub Issues](https://github.com/YOUR_USERNAME/benchmark-spark/issues)
- **Discussions:** [GitHub Discussions](https://github.com/YOUR_USERNAME/benchmark-spark/discussions)
- **Documentation:** [docs/](https://github.com/YOUR_USERNAME/benchmark-spark/tree/main/docs)

---

## ğŸ‰ Quick Links

- [Quick Start Guide](../QUICKSTART.html)
- [Setup Instructions](setup.html)
- [Methodology Details](methodology.html)
- [Reference Paper Summary](reference_paper.html)
- [GitHub Repository](https://github.com/YOUR_USERNAME/benchmark-spark)

---

<div style="text-align: center; margin-top: 50px; padding: 20px; background-color: #f0f0f0;">
<p><strong>Ready to benchmark your DGX Spark?</strong></p>
<p><a href="https://github.com/YOUR_USERNAME/benchmark-spark" style="background-color: #76B900; color: white; padding: 10px 20px; text-decoration: none; border-radius: 5px; font-weight: bold;">Get Started Now â†’</a></p>
</div>
