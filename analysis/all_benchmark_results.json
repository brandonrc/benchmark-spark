{
  "models": {
    "deepseek": {
      "native": [
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 10,
          "timestamp": "20251109_042404",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9308,
          "output_throughput": 119.1443,
          "latency": null,
          "peak_memory": 70.05,
          "kv_cache": 44.69
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 1,
          "timestamp": "20251109_022626",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9428,
          "output_throughput": 120.6821,
          "latency": null,
          "peak_memory": 69.28,
          "kv_cache": 45.38
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 2,
          "timestamp": "20251109_023928",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9413,
          "output_throughput": 120.492,
          "latency": null,
          "peak_memory": 72.59,
          "kv_cache": 42.41
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 3,
          "timestamp": "20251109_025212",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9402,
          "output_throughput": 120.3445,
          "latency": null,
          "peak_memory": 71.79,
          "kv_cache": 43.13
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 4,
          "timestamp": "20251109_030505",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9388,
          "output_throughput": 120.1671,
          "latency": null,
          "peak_memory": 70.75,
          "kv_cache": 44.06
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 5,
          "timestamp": "20251109_031756",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.935,
          "output_throughput": 119.6823,
          "latency": null,
          "peak_memory": 70.17,
          "kv_cache": 44.59
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 6,
          "timestamp": "20251109_033113",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9334,
          "output_throughput": 119.477,
          "latency": null,
          "peak_memory": 69.9,
          "kv_cache": 44.82
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 7,
          "timestamp": "20251109_034448",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9305,
          "output_throughput": 119.1005,
          "latency": null,
          "peak_memory": 69.93,
          "kv_cache": 44.8
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 8,
          "timestamp": "20251109_035741",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9327,
          "output_throughput": 119.38,
          "latency": null,
          "peak_memory": 69.97,
          "kv_cache": 44.77
        },
        {
          "environment": "native",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 9,
          "timestamp": "20251109_041102",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9328,
          "output_throughput": 119.3999,
          "latency": null,
          "peak_memory": 70.27,
          "kv_cache": 44.49
        }
      ],
      "container": [
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 10,
          "timestamp": "20251109_065229",
          "start_temp_celsius": 40,
          "end_temp_celsius": 57,
          "exit_code": 0,
          "throughput": 0.9305,
          "output_throughput": 119.1019,
          "latency": null,
          "peak_memory": 101.56,
          "kv_cache": 16.33
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 1,
          "timestamp": "20251109_043700",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9341,
          "output_throughput": 119.5678,
          "latency": null,
          "peak_memory": 104.41,
          "kv_cache": 13.77
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 2,
          "timestamp": "20251109_045200",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9329,
          "output_throughput": 119.411,
          "latency": null,
          "peak_memory": 103.8,
          "kv_cache": 14.32
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 3,
          "timestamp": "20251109_050656",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9322,
          "output_throughput": 119.3257,
          "latency": null,
          "peak_memory": 101.19,
          "kv_cache": 16.67
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 4,
          "timestamp": "20251109_052155",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9337,
          "output_throughput": 119.5146,
          "latency": null,
          "peak_memory": 101.12,
          "kv_cache": 16.73
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 5,
          "timestamp": "20251109_053705",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9332,
          "output_throughput": 119.4442,
          "latency": null,
          "peak_memory": 99.91,
          "kv_cache": 17.82
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 6,
          "timestamp": "20251109_055221",
          "start_temp_celsius": 40,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9321,
          "output_throughput": 119.3084,
          "latency": null,
          "peak_memory": 101.15,
          "kv_cache": 16.7
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 7,
          "timestamp": "20251109_060717",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9319,
          "output_throughput": 119.2852,
          "latency": null,
          "peak_memory": 99.93,
          "kv_cache": 17.8
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 8,
          "timestamp": "20251109_062229",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.932,
          "output_throughput": 119.3001,
          "latency": null,
          "peak_memory": 100.17,
          "kv_cache": 17.58
        },
        {
          "environment": "container",
          "model_name": "deepseek",
          "model_id": "deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "model_path": "/data/models/huggingface/deepseek-ai/DeepSeek-R1-Distill-Qwen-7B",
          "iteration": 9,
          "timestamp": "20251109_063725",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9352,
          "output_throughput": 119.7047,
          "latency": null,
          "peak_memory": 99.76,
          "kv_cache": 17.95
        }
      ]
    },
    "gpt120b": {
      "native": [
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 10,
          "timestamp": "20251108_234312",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9385,
          "output_throughput": 120.1234,
          "latency": null,
          "peak_memory": 70.21,
          "kv_cache": 44.55
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 1,
          "timestamp": "20251108_195154",
          "start_temp_celsius": 38,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9441,
          "output_throughput": 120.8445,
          "latency": null,
          "peak_memory": 74.23,
          "kv_cache": 40.93
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 1,
          "timestamp": "20251108_201941",
          "start_temp_celsius": 38,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9408,
          "output_throughput": 120.4272,
          "latency": null,
          "peak_memory": 74.8,
          "kv_cache": 40.42
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 1,
          "timestamp": "20251108_203930",
          "start_temp_celsius": 38,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.942,
          "output_throughput": 120.5738,
          "latency": null,
          "peak_memory": 74.83,
          "kv_cache": 40.39
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 1,
          "timestamp": "20251108_214625",
          "start_temp_celsius": 39,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.94,
          "output_throughput": 120.3165,
          "latency": null,
          "peak_memory": 72.31,
          "kv_cache": 42.66
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 2,
          "timestamp": "20251108_215917",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9399,
          "output_throughput": 120.3017,
          "latency": null,
          "peak_memory": 72.11,
          "kv_cache": 42.83
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 3,
          "timestamp": "20251108_221158",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9386,
          "output_throughput": 120.137,
          "latency": null,
          "peak_memory": 72.53,
          "kv_cache": 42.46
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 4,
          "timestamp": "20251108_222443",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9365,
          "output_throughput": 119.8666,
          "latency": null,
          "peak_memory": 71.54,
          "kv_cache": 43.35
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 5,
          "timestamp": "20251108_223812",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9349,
          "output_throughput": 119.6683,
          "latency": null,
          "peak_memory": 69.89,
          "kv_cache": 44.84
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 6,
          "timestamp": "20251108_225127",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9379,
          "output_throughput": 120.0483,
          "latency": null,
          "peak_memory": 70.1,
          "kv_cache": 44.65
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 7,
          "timestamp": "20251108_230443",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9433,
          "output_throughput": 120.745,
          "latency": null,
          "peak_memory": 70.28,
          "kv_cache": 44.48
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 8,
          "timestamp": "20251108_231735",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9394,
          "output_throughput": 120.2371,
          "latency": null,
          "peak_memory": 70.09,
          "kv_cache": 44.65
        },
        {
          "environment": "native",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 9,
          "timestamp": "20251108_233016",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9387,
          "output_throughput": 120.1555,
          "latency": null,
          "peak_memory": 69.47,
          "kv_cache": 45.21
        }
      ],
      "container": [
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 10,
          "timestamp": "20251109_021130",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.942,
          "output_throughput": 120.5727,
          "latency": null,
          "peak_memory": 93.24,
          "kv_cache": 23.82
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 1,
          "timestamp": "20251108_235609",
          "start_temp_celsius": 40,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9495,
          "output_throughput": 121.5415,
          "latency": null,
          "peak_memory": 100.41,
          "kv_cache": 17.37
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 2,
          "timestamp": "20251109_001115",
          "start_temp_celsius": 39,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9425,
          "output_throughput": 120.6387,
          "latency": null,
          "peak_memory": 93.79,
          "kv_cache": 23.32
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 3,
          "timestamp": "20251109_002608",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.937,
          "output_throughput": 119.936,
          "latency": null,
          "peak_memory": 92.9,
          "kv_cache": 24.13
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 4,
          "timestamp": "20251109_004056",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9386,
          "output_throughput": 120.1362,
          "latency": null,
          "peak_memory": 92.28,
          "kv_cache": 24.69
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 5,
          "timestamp": "20251109_005559",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9388,
          "output_throughput": 120.1717,
          "latency": null,
          "peak_memory": 92.29,
          "kv_cache": 24.67
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 6,
          "timestamp": "20251109_011044",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9377,
          "output_throughput": 120.021,
          "latency": null,
          "peak_memory": 92.67,
          "kv_cache": 24.33
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 7,
          "timestamp": "20251109_012550",
          "start_temp_celsius": 40,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9341,
          "output_throughput": 119.5681,
          "latency": null,
          "peak_memory": 92.76,
          "kv_cache": 24.26
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 8,
          "timestamp": "20251109_014110",
          "start_temp_celsius": 39,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9418,
          "output_throughput": 120.5486,
          "latency": null,
          "peak_memory": 91.91,
          "kv_cache": 25.02
        },
        {
          "environment": "container",
          "model_name": "gpt120b",
          "model_id": "openai/gpt-oss-120b",
          "model_path": "/data/models/huggingface/openai/gpt-oss-120b",
          "iteration": 9,
          "timestamp": "20251109_015616",
          "start_temp_celsius": 40,
          "end_temp_celsius": 57,
          "exit_code": 0,
          "throughput": 0.9448,
          "output_throughput": 120.9339,
          "latency": null,
          "peak_memory": 92.04,
          "kv_cache": 24.9
        }
      ]
    },
    "qwen72b": {
      "native": [
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 10,
          "timestamp": "20251109_090519",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9318,
          "output_throughput": 119.2659,
          "latency": null,
          "peak_memory": 70.56,
          "kv_cache": 44.24
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 1,
          "timestamp": "20251109_070740",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9289,
          "output_throughput": 118.9032,
          "latency": null,
          "peak_memory": 69.12,
          "kv_cache": 45.53
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 2,
          "timestamp": "20251109_072123",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9334,
          "output_throughput": 119.4692,
          "latency": null,
          "peak_memory": 70.19,
          "kv_cache": 44.57
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 3,
          "timestamp": "20251109_073422",
          "start_temp_celsius": 40,
          "end_temp_celsius": 60,
          "exit_code": 0,
          "throughput": 0.9299,
          "output_throughput": 119.0307,
          "latency": null,
          "peak_memory": 70.19,
          "kv_cache": 44.57
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 4,
          "timestamp": "20251109_074717",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9306,
          "output_throughput": 119.1215,
          "latency": null,
          "peak_memory": 70.07,
          "kv_cache": 44.67
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 5,
          "timestamp": "20251109_080047",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9329,
          "output_throughput": 119.4126,
          "latency": null,
          "peak_memory": 70.04,
          "kv_cache": 44.7
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 6,
          "timestamp": "20251109_081351",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9352,
          "output_throughput": 119.7004,
          "latency": null,
          "peak_memory": 70.0,
          "kv_cache": 44.74
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 7,
          "timestamp": "20251109_082647",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9363,
          "output_throughput": 119.8507,
          "latency": null,
          "peak_memory": 70.13,
          "kv_cache": 44.62
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 8,
          "timestamp": "20251109_083939",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9312,
          "output_throughput": 119.1899,
          "latency": null,
          "peak_memory": 70.06,
          "kv_cache": 44.68
        },
        {
          "environment": "native",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 9,
          "timestamp": "20251109_085231",
          "start_temp_celsius": 40,
          "end_temp_celsius": 61,
          "exit_code": 0,
          "throughput": 0.9324,
          "output_throughput": 119.3467,
          "latency": null,
          "peak_memory": 69.94,
          "kv_cache": 44.79
        }
      ],
      "container": [
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 10,
          "timestamp": "20251109_113358",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9318,
          "output_throughput": 119.2728,
          "latency": null,
          "peak_memory": 87.43,
          "kv_cache": 29.05
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 1,
          "timestamp": "20251109_091813",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9369,
          "output_throughput": 119.9279,
          "latency": null,
          "peak_memory": 102.93,
          "kv_cache": 15.1
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 2,
          "timestamp": "20251109_093302",
          "start_temp_celsius": 40,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9359,
          "output_throughput": 119.7945,
          "latency": null,
          "peak_memory": 97.11,
          "kv_cache": 20.34
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 3,
          "timestamp": "20251109_094755",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9337,
          "output_throughput": 119.5092,
          "latency": null,
          "peak_memory": 92.28,
          "kv_cache": 24.69
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 4,
          "timestamp": "20251109_100238",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9322,
          "output_throughput": 119.3204,
          "latency": null,
          "peak_memory": 88.1,
          "kv_cache": 28.44
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 5,
          "timestamp": "20251109_101804",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9329,
          "output_throughput": 119.4081,
          "latency": null,
          "peak_memory": 85.21,
          "kv_cache": 31.05
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 6,
          "timestamp": "20251109_103317",
          "start_temp_celsius": 40,
          "end_temp_celsius": 58,
          "exit_code": 0,
          "throughput": 0.9334,
          "output_throughput": 119.4765,
          "latency": null,
          "peak_memory": 86.33,
          "kv_cache": 30.04
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 7,
          "timestamp": "20251109_104840",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9342,
          "output_throughput": 119.5825,
          "latency": null,
          "peak_memory": 87.25,
          "kv_cache": 29.21
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 8,
          "timestamp": "20251109_110357",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9332,
          "output_throughput": 119.4499,
          "latency": null,
          "peak_memory": 87.56,
          "kv_cache": 28.93
        },
        {
          "environment": "container",
          "model_name": "qwen72b",
          "model_id": "Qwen/Qwen2.5-72B-Instruct",
          "model_path": "/data/models/huggingface/Qwen/Qwen2.5-72B-Instruct",
          "iteration": 9,
          "timestamp": "20251109_111844",
          "start_temp_celsius": 40,
          "end_temp_celsius": 59,
          "exit_code": 0,
          "throughput": 0.9323,
          "output_throughput": 119.3373,
          "latency": null,
          "peak_memory": 85.99,
          "kv_cache": 30.35
        }
      ]
    }
  }
}