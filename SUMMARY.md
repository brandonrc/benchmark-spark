# Benchmark Repository Summary

## üìã What Has Been Created

This repository contains a complete, production-ready benchmarking suite for comparing GPU performance between containerized and native execution on DGX Spark systems.

## üéØ Purpose

Investigate the ~50% performance degradation observed when using NVIDIA's `spark-single-gpu-dev` container by:
1. Quantifying container overhead precisely
2. Comparing against native execution baseline
3. Providing reproducible, shareable results
4. Identifying whether containerization is the bottleneck

## üì¶ Repository Contents

### Core Benchmark Scripts

#### `benchmarks/simple_matmul.py`
- Matrix multiplication benchmark (10,000 x 10,000)
- Sanity check for basic GPU operations
- Replicates methodology from reference paper
- Outputs: CSV with execution times, statistics

#### `benchmarks/llm_inference.py`
- TensorRT-LLM inference benchmark
- Configurable batch sizes and sequence lengths
- Measures throughput (tokens/sec) and latency (ms)
- Supports both real and simulated workloads
- Outputs: Detailed CSV with per-run metrics

#### `benchmarks/config.yaml`
- Centralized test configuration
- 6 pre-defined test scenarios
- Easy to customize parameters
- Controls iterations, warm-up, cool-down

### Automation Scripts

#### `scripts/run_container_benchmark.sh`
- Automated Docker container testing
- GPU monitoring integration
- Supports individual or all benchmarks
- Error handling and logging
- Color-coded output for clarity

#### `scripts/run_native_benchmark.sh`
- Automated native environment testing
- Parallel GPU monitoring
- Environment activation handling
- Matches container test methodology

#### `scripts/setup_native.sh`
- One-command native environment setup
- Installs PyTorch, TensorFlow, TensorRT-LLM
- Builds from source with error checking
- Creates activation script for easy reuse
- Estimated time: 30-60 minutes

### Analysis Tools

#### `analysis/compare_results.py`
- Statistical comparison (t-tests, confidence intervals)
- Overhead calculation
- Automated visualization generation
- Markdown report generation
- Handles both matmul and LLM results

### Documentation

#### `README.md`
- Project overview and motivation
- Quick start instructions
- Repository structure
- Expected outcomes guide

#### `QUICKSTART.md`
- 5-minute start guide
- Step-by-step commands
- Troubleshooting tips
- Results interpretation

#### `docs/setup.md`
- Comprehensive setup guide
- Prerequisites checklist
- Docker and native installation
- Troubleshooting section
- Configuration options

#### `docs/methodology.md`
- Scientific methodology
- Experimental design
- Statistical analysis approach
- Quality assurance procedures
- Reproducibility guidelines

#### `docs/reference_paper.md`
- Summary of academic research
- Key findings from prior work
- Comparison to our approach
- Expected differences on DGX hardware

#### `docs/index.md`
- GitHub Pages landing page
- Quick navigation
- Visual workflow diagram
- Results interpretation guide

## üîÑ Complete Workflow

### Phase 1: Container Testing (5-10 minutes)
```bash
./scripts/run_container_benchmark.sh all
```
- Pulls NVIDIA container
- Runs matmul and LLM benchmarks
- Monitors GPU metrics
- Saves results to `results/container/`

### Phase 2: Native Setup (30-60 minutes, one-time)
```bash
./scripts/setup_native.sh
```
- Checks prerequisites
- Creates virtual environment
- Installs PyTorch and TensorFlow
- Builds TensorRT-LLM from source
- Creates activation script

### Phase 3: Native Testing (5-10 minutes)
```bash
source ~/tensorrt-llm/activate.sh
./scripts/run_native_benchmark.sh all
```
- Runs identical benchmarks natively
- Monitors GPU metrics
- Saves results to `results/native/`

### Phase 4: Analysis (2-5 minutes)
```bash
python analysis/compare_results.py
```
- Loads results from both environments
- Performs statistical analysis
- Generates comparison plots
- Creates markdown report
- Outputs to `results/comparison/`

## üìä Outputs Generated

### CSV Data Files
- `matmul_results.csv` - Raw execution times
- `matmul_results_summary.csv` - Statistical summary
- `llm_benchmark_*.csv` - Detailed per-run data
- `llm_benchmark_*_summary.csv` - Aggregated statistics
- `gpu_metrics_*.csv` - nvidia-smi monitoring data

### Visualizations
- `matmul_comparison.png` - Box plots and line charts
- `llm_comparison.png` - Throughput bars and overhead graphs

### Reports
- `comparison_report.md` - Executive summary with interpretation

## üéØ Key Metrics Measured

### Performance Metrics
1. **Execution Time** (seconds) - End-to-end latency
2. **Throughput** (tokens/second) - LLM inference rate
3. **Latency** (milliseconds) - Per-request timing
4. **Overhead** (percentage) - Container vs native difference

### GPU Metrics
1. **GPU Utilization** (%) - Compute usage
2. **Memory Utilization** (%) - VRAM usage
3. **Temperature** (¬∞C) - Thermal state
4. **Power Draw** (W) - Energy consumption
5. **Clock Speeds** (MHz) - GPU/memory clocks

### Statistical Metrics
- Mean, median, standard deviation
- Min, max, percentiles (P50, P95, P99)
- Coefficient of variation
- T-test p-values
- 95% confidence intervals

## ‚úÖ Quality Features

### Reproducibility
- ‚úÖ Fixed random seeds (42)
- ‚úÖ Version-controlled configurations
- ‚úÖ Documented environment specs
- ‚úÖ Automated execution (no manual steps)

### Statistical Rigor
- ‚úÖ Warm-up iterations (10)
- ‚úÖ Measurement iterations (100)
- ‚úÖ Cool-down periods (30s)
- ‚úÖ Significance testing (Œ±=0.05)

### Robustness
- ‚úÖ Error handling in all scripts
- ‚úÖ Prerequisite checking
- ‚úÖ GPU availability validation
- ‚úÖ Graceful failure messages

### Usability
- ‚úÖ Color-coded terminal output
- ‚úÖ Progress indicators
- ‚úÖ Clear error messages
- ‚úÖ Comprehensive documentation

## üöÄ Next Steps for User

1. **Initial Run**
   ```bash
   git clone <repo>
   cd benchmark-spark
   ./scripts/run_container_benchmark.sh all
   ```

2. **Review Initial Results**
   - Check CSV files in `results/container/`
   - Verify GPU metrics look reasonable
   - Confirm no errors occurred

3. **Setup Native** (if container shows issues)
   ```bash
   ./scripts/setup_native.sh
   ```

4. **Compare Environments**
   ```bash
   source ~/tensorrt-llm/activate.sh
   ./scripts/run_native_benchmark.sh all
   python analysis/compare_results.py
   ```

5. **Analyze Results**
   - Read `results/comparison/comparison_report.md`
   - View plots in `results/comparison/`
   - Determine if container is the bottleneck

6. **Share Findings**
   - Commit results to repo
   - Enable GitHub Pages
   - Share with NVIDIA if needed

## üìà Expected Outcomes

### Scenario 1: Container Overhead < 10%
**Interpretation:** Container is NOT the problem
**Action:** Investigate drivers, thermal, power, configuration

### Scenario 2: Container Overhead 20-40%
**Interpretation:** Normal containerization cost
**Action:** Balance convenience vs performance needs

### Scenario 3: Container Overhead > 50%
**Interpretation:** Container is likely the problem
**Action:** Test production container, report to NVIDIA

## üîß Customization Options

### Adjust Test Configurations
Edit `benchmarks/config.yaml`:
- Add/remove test cases
- Change batch sizes
- Modify sequence lengths
- Adjust iteration counts

### Use Different Container
```bash
export CONTAINER_IMAGE="nvcr.io/nvidia/tensorrt-llm:latest"
./scripts/run_container_benchmark.sh
```

### Modify GPU Monitoring Rate
```bash
export GPU_MONITOR_INTERVAL=0.5  # Sample every 0.5s
./scripts/run_container_benchmark.sh
```

### Custom Installation Location
```bash
export INSTALL_DIR="$HOME/my-custom-path"
./scripts/setup_native.sh
```

## üìö Technical Details

### Dependencies
- **Docker:** Container runtime
- **NVIDIA Container Toolkit:** GPU passthrough
- **CUDA 12.x:** GPU computing
- **PyTorch 2.x:** Deep learning framework
- **TensorFlow 2.x:** ML framework
- **TensorRT-LLM:** LLM inference engine
- **Python 3.10+:** Scripting language

### System Requirements
- **GPU:** NVIDIA GPU with CUDA support (12GB+ VRAM)
- **RAM:** 32GB+ recommended
- **Disk:** 50GB+ free space (for builds)
- **OS:** Linux (Ubuntu 22.04 recommended)

### Time Requirements
- Container benchmark: 5-10 minutes
- Native setup: 30-60 minutes (one-time)
- Native benchmark: 5-10 minutes
- Analysis: 2-5 minutes
- **Total first run:** ~1 hour
- **Subsequent runs:** ~15 minutes

## üéì Scientific Basis

Based on published research:
- Sani et al. (2025) - GPU passthrough benchmarking
- Shetty et al. (2017) - Docker performance evaluation
- NVIDIA documentation - Best practices

Improvements over prior work:
- Enterprise GPU hardware
- Production LLM workloads
- Larger sample sizes (100 vs 10 iterations)
- Statistical significance testing
- Hardware-specific containers

## ‚ú® Unique Features

1. **DGX-Specific:** Optimized for DGX Spark hardware
2. **Production-Ready:** Real LLM inference workloads
3. **Fully Automated:** One command to run everything
4. **Statistically Rigorous:** T-tests, confidence intervals
5. **Well-Documented:** Comprehensive guides and methodology
6. **Open Source:** MIT licensed, community-driven
7. **Reproducible:** Fixed seeds, version control
8. **Shareable:** GitHub Pages integration

## üéâ Success Criteria

This repository enables you to:
- ‚úÖ Run reproducible GPU benchmarks
- ‚úÖ Quantify container overhead precisely
- ‚úÖ Identify performance bottlenecks
- ‚úÖ Generate publication-quality results
- ‚úÖ Share findings with stakeholders
- ‚úÖ Make informed architecture decisions

## üìÑ Files Summary

**Total Files Created:** 15+

- 3 Benchmark scripts (Python)
- 3 Automation scripts (Bash)
- 5 Documentation files (Markdown)
- 1 Configuration file (YAML)
- 1 Analysis script (Python)
- 1 Requirements file
- 1 License file
- 1 Gitignore file

**Lines of Code:** ~3000+

## üèÜ Ready to Use

This repository is **production-ready** and can be:
- Cloned and run immediately
- Customized for specific needs
- Extended with new benchmarks
- Deployed in CI/CD pipelines
- Published to GitHub Pages
- Shared with NVIDIA support

**No additional setup required beyond running the provided scripts.**
